# LangGraph workflow configuration with LLM model settings
models:
  # Default model configuration used as fallback
  default:
    provider: deepseek  # Can use: openai, deepseek, openrouter
    model_name: "deepseek-chat"
    temperature: 0.1
    max_tokens: 4000
    
  # Examples of other providers:
  # OpenAI:
  #   provider: openai
  #   model_name: "gpt-4.1-mini"
  
  # DeepSeek Reasoner (without structured output):
  #   provider: deepseek
  #   model_name: "deepseek-reasoner"
  #   max_tokens: 32000

  nodes:
    input_processing:
      provider: deepseek
      model_name: "deepseek-chat"
      temperature: 0.1
      max_tokens: 2000
      requires_structured_output: false
    
    generating_content:
      provider: deepseek
      model_name: "deepseek-chat"
      temperature: 0.5
      max_tokens: 8000
      requires_structured_output: false
    
    recognition_handwritten:
      # OpenAI gpt-4o-mini
      provider: openai
      model_name: "gpt-4o-mini"
      temperature: 0.1
      max_tokens: 6000
      requires_structured_output: false
      requires_vision: true
    
    synthesis_material:
      provider: deepseek
      model_name: "deepseek-chat"
      temperature: 0.5
      max_tokens: 8000
      requires_structured_output: false
    
    generating_questions:
      # OpenAI gpt-4o-mini - for structured output (generating questions)
      provider: openai
      model_name: "gpt-4o-mini"
      temperature: 0.0
      max_tokens: 6000
      requires_structured_output: true
    
    answer_question:
      provider: deepseek
      model_name: "deepseek-chat"
      temperature: 0.2
      max_tokens: 8000
      requires_structured_output: false

    edit_material:
      # OpenAI gpt-4o-mini - for structured output (editing material)
      provider: openai
      model_name: "gpt-4o-mini"
      temperature: 0.2
      max_tokens: 6000
      requires_structured_output: true

    security_guard:
    # OpenAI gpt-4o-mini - for security guard

      provider: openai
      model_name: "gpt-4o-mini"
      temperature: 0.2
      max_tokens: 6000
      requires_structured_output: false